{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPMoTM5STboM7aVN4j+Pwxy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pitthexai/ICHI2023_EBAIC/blob/main/EBAIC2024_Workshop/Track02_Medical-Imaging-Informatics/EBAIC2024_KneeSegmentationBiasEvaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bony Anatomy Segmentation"
      ],
      "metadata": {
        "id": "c9FKzHsdt53U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "s93SFhrOrxTa",
        "outputId": "2c560e18-db7c-42a6-829b-66f7b7e58f8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting segmentation-models-pytorch\n",
            "  Downloading segmentation_models_pytorch-0.3.3-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.3.1)\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydicom\n",
            "  Downloading pydicom-2.4.4-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (4.0.2)\n",
            "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.18.0+cu121)\n",
            "Collecting pretrainedmodels==0.7.4 (from segmentation-models-pytorch)\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting efficientnet-pytorch==0.7.1 (from segmentation-models-pytorch)\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting timm==0.9.2 (from segmentation-models-pytorch)\n",
            "  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (4.66.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (9.4.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.3.0+cu121)\n",
            "Collecting munch (from pretrainedmodels==0.7.4->segmentation-models-pytorch)\n",
            "  Downloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation-models-pytorch) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation-models-pytorch) (0.23.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation-models-pytorch) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.11.4)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.19.3)\n",
            "Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.4)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.9.0.80)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.0)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nibabel) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.11.0)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from qudida>=0.0.4->albumentations) (1.2.2)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (3.3)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (2024.5.22)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (1.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (2.31.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (2024.2.2)\n",
            "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16429 sha256=5605b96a8ddfe4e18f082618367e42d8a2360179babd3f3681dacc321fa1cc7e\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60945 sha256=ab6ad6b1777e3a75a413794703e495f0985f1f42d91b22ffd8bac50703bed588\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\n",
            "Successfully built efficientnet-pytorch pretrainedmodels\n",
            "Installing collected packages: pydicom, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, munch, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics, efficientnet-pytorch, timm, pretrainedmodels, segmentation-models-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.7.1 lightning-utilities-0.11.2 munch-4.0.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 pretrainedmodels-0.7.4 pydicom-2.4.4 segmentation-models-pytorch-0.3.3 timm-0.9.2 torchmetrics-1.4.0.post0\n"
          ]
        }
      ],
      "source": [
        "!pip install segmentation-models-pytorch albumentations torchmetrics pydicom nibabel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "import copy\n",
        "\n",
        "from zipfile import ZipFile\n",
        "\n",
        "from io import BytesIO\n",
        "from gzip import GzipFile\n",
        "\n",
        "import os\n",
        "\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "import pydicom\n",
        "import nibabel\n",
        "from nibabel import FileHolder, Nifti1Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "import segmentation_models_pytorch as smp\n",
        "from segmentation_models_pytorch import utils as smp_utils\n",
        "\n",
        "from torchmetrics.classification import MulticlassJaccardIndex, Dice\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Pd3G4QOJs0KE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "yevZb-zWr5HB",
        "outputId": "d0429287-33a9-4c16-e477-87ae6b78564c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_STATE = 42"
      ],
      "metadata": {
        "id": "Hqi8DEP2sI34"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "8rjWsBFbt2Vh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directories for\n",
        "\n",
        "filename = \"knee_seg_sample.csv\"\n",
        "directory = \"KneeSample\"\n",
        "zipfile = \"knee_sample.zip\"\n",
        "\n",
        "classes = {\n",
        "    0: \"Background\",\n",
        "    1: \"R Patella\",\n",
        "    2: \"R Femur\",\n",
        "    3: \"R Tibia\",\n",
        "    4: \"R Fibula\",\n",
        "    5: \"L Patella\",\n",
        "    6: \"L Femur\",\n",
        "    7: \"L Tibia\",\n",
        "    8: \"L Fibula\"\n",
        "}\n",
        "\n",
        "zipfile_loc = f\"/content/drive/MyDrive/GoogleColabProjects/EBAIC_2024/KneeDataset/{zipfile}\"\n",
        "data_location = f\"/content/data/{directory}/{filename}\"\n",
        "data_save_location = f\"/content/data/{directory}/\""
      ],
      "metadata": {
        "id": "bc8qVtmMsRB0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M9YCLP7atBq9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(data_save_location):\n",
        "    with ZipFile(zipfile_loc, 'r') as zipf:\n",
        "        zipf.extractall(\"/content/\")"
      ],
      "metadata": {
        "id": "p3ZWmq51susS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_train_test_split(csv_pth, filter_query=None):\n",
        "    data_records = pd.read_csv(csv_pth)\n",
        "    data_records = data_records[data_records.id != 9025994].reset_index(drop=True)\n",
        "    if filter_query:\n",
        "        data_records = data_records.query(filter_query)\n",
        "\n",
        "    train, test = train_test_split(data_records.id.unique(), test_size=0.3, random_state=42)\n",
        "    valid, test = train_test_split(test, test_size=0.5, random_state=42)\n",
        "\n",
        "    train = data_records[data_records.id.isin(train)].reset_index(drop=True)\n",
        "    valid = data_records[data_records.id.isin(valid)].reset_index(drop=True)\n",
        "    test = data_records[data_records.id.isin(test)].reset_index(drop=True)\n",
        "\n",
        "    return train, valid, test"
      ],
      "metadata": {
        "id": "e2Qwhgqysxrw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def balance_dataset(data, by_filter1, by_filter2):\n",
        "    filtered1 = data.query(by_filter1)\n",
        "    filtered2 = data.query(by_filter2)\n",
        "\n",
        "    min_sample_size = np.minimum(len(filtered1), len(filtered2))\n",
        "    samp1 = filtered1.sample(min_sample_size,  random_state = 42)\n",
        "    samp2 = filtered2.sample(min_sample_size,  random_state = 42)\n",
        "\n",
        "    balanced_data = pd.concat([samp1, samp2]).reset_index(drop=True)\n",
        "\n",
        "    print(f\"Training dataset reduced from size of {len(data)} samples to a balanced dataset of size {len(balanced_data)} samples\")\n",
        "\n",
        "    return balanced_data"
      ],
      "metadata": {
        "id": "eagrBr6TtHxt"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Baseline Datasets\n",
        "train_all, valid_all, test_all = generate_train_test_split(data_location)\n",
        "train_white, valid_white, test_white = generate_train_test_split(data_location, filter_query=\"P02RACE == '1: White or Caucasian'\")\n",
        "train_black, valid_black, test_black = generate_train_test_split(data_location, filter_query=\"P02RACE == '2: Black or African American'\")\n",
        "train_male, valid_male, test_male = generate_train_test_split(data_location, filter_query=\"P02SEX == '1: Male'\")\n",
        "train_female, valid_female, test_female = generate_train_test_split(data_location, filter_query=\"P02SEX == '2: Female'\")\n",
        "\n",
        "balanced_gender_train = balance_dataset(train_all, \"P02SEX == '1: Male'\", \"P02SEX == '2: Female'\")\n",
        "balanced_race_train = balance_dataset(train_all, \"P02RACE == '1: White or Caucasian'\", \"P02RACE == '2: Black or African American'\")"
      ],
      "metadata": {
        "id": "G1e695BotJZB",
        "outputId": "80a6db39-c53e-4cfe-c0e2-adb1ad9b538e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset reduced from size of 70 samples to a balanced dataset of size 60 samples\n",
            "Training dataset reduced from size of 70 samples to a balanced dataset of size 62 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline Models\n"
      ],
      "metadata": {
        "id": "ZFxmkKsot_9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BonyAnatomyJointSegmentationDataset(Dataset):\n",
        "    def __init__(self, root_dir, ids, num_classes, transforms=None, preprocessing=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.pids = ids\n",
        "        self.num_classes = num_classes\n",
        "        self.transforms = transforms\n",
        "        self.preprocessing = preprocessing\n",
        "\n",
        "    def load_dicom(self, path):\n",
        "        dicom_img = pydicom.dcmread(path)\n",
        "        return dicom_img.pixel_array.astype(np.float32)\n",
        "\n",
        "    def load_nii(self, path):\n",
        "        nii_annot = nibabel.load(path)\n",
        "        nii_annot_data = nii_annot.get_fdata()\n",
        "\n",
        "        if len(nii_annot_data.shape) == 3 and nii_annot_data.shape[-1] > 1:\n",
        "            if nii_annot_data.shape[-1] == 2:\n",
        "                nii_annot_data = nii_annot_data[:, :, 1]\n",
        "            else:\n",
        "                nii_annot_data = nii_annot_data[:, :, nii_annot_data.shape[-1]//2]\n",
        "\n",
        "            nii_annot_data = np.expand_dims(nii_annot_data, axis=-1)\n",
        "\n",
        "\n",
        "        nii_annot_data = cv2.rotate(nii_annot_data, cv2.ROTATE_90_CLOCKWISE)\n",
        "        nii_annot_data = cv2.flip(nii_annot_data, 1)\n",
        "        return nii_annot_data\n",
        "\n",
        "    def get_file_path(self, filename):\n",
        "        return os.path.join(self.root_dir, filename)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        image = self.load_dicom(self.get_file_path(os.path.join(\"Images/\", str(self.pids[idx]) + \".dcm\")))\n",
        "        mask = self.load_nii(self.get_file_path(os.path.join(\"Annotations\", str(self.pids[idx]) + \".nii.gz\")))\n",
        "\n",
        "#         if len(np.unique(mask)) != self.num_classes:\n",
        "#             print(self.pids[idx])\n",
        "        if self.transforms is not None:\n",
        "            transformed = self.transforms(image=image, mask=mask)\n",
        "            image = transformed[\"image\"]\n",
        "            mask = transformed[\"mask\"]\n",
        "        if self.preprocessing is not None:\n",
        "            transformed = self.preprocessing(image=image, mask=mask)\n",
        "            image = transformed[\"image\"]\n",
        "            mask = transformed[\"mask\"]\n",
        "\n",
        "        return image.type(torch.FloatTensor), mask.long()\n"
      ],
      "metadata": {
        "id": "RoQs24RmuUKX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(train_loader, valid_loader, num_classes=9):\n",
        "    encoder = \"resnet18\"\n",
        "    encoder_weights = \"imagenet\"\n",
        "    activation = None\n",
        "\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = copy.deepcopy(smp.Unet(encoder_name=encoder, encoder_weights=encoder_weights, in_channels=1,\n",
        "                    classes=num_classes, activation=activation)).to(device)\n",
        "    model.encoder.requires_grad_ = False\n",
        "    model.decoder.requires_grad_ = False\n",
        "\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "    loss.__name__=\" loss\"\n",
        "\n",
        "    multi_jaccard = MulticlassJaccardIndex(num_classes=num_classes, average=\"macro\").to(device)\n",
        "    multi_jaccard.__name__ = \"iou_score\"\n",
        "    metrics = [multi_jaccard]\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-04)\n",
        "\n",
        "    # create epoch runners\n",
        "    # it is a simple loop of iterating over dataloader`s samples\n",
        "    train_epoch = smp.utils.train.TrainEpoch(\n",
        "        model,\n",
        "        loss=loss,\n",
        "        metrics=metrics,\n",
        "        optimizer=optimizer,\n",
        "        device=device,\n",
        "        verbose=True,\n",
        "    )\n",
        "\n",
        "    valid_epoch = smp.utils.train.ValidEpoch(\n",
        "        model,\n",
        "        loss=loss,\n",
        "        metrics=metrics,\n",
        "        device=device,\n",
        "        verbose=True,\n",
        "    )\n",
        "\n",
        "    max_score = 0\n",
        "\n",
        "    for i in range(1, 51):\n",
        "\n",
        "        print('\\nEpoch: {}'.format(i))\n",
        "        train_logs = train_epoch.run(train_loader)\n",
        "        valid_logs = valid_epoch.run(valid_loader)\n",
        "\n",
        "        # do something (save model, change lr, etc.)\n",
        "        if max_score < valid_logs['iou_score']:\n",
        "            max_score = valid_logs['iou_score']\n",
        "            torch.save(model, './best_model.pth')\n",
        "            print('Model saved!')\n",
        "\n",
        "    # Return best model\n",
        "    model = torch.load('./best_model.pth')\n",
        "    return model"
      ],
      "metadata": {
        "id": "EvQi5plPvfu3"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, test_loader, class_labels):\n",
        "    model.eval()\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    fig, ax = plt.subplots(nrows=5, ncols=3, figsize=(10, 10), sharex=True, sharey=True)\n",
        "\n",
        "    x, y = next(iter(test_loader))\n",
        "    out = torch.softmax(model(x.to(device)), dim=1)\n",
        "    #out = out.detach().cpu().numpy()\n",
        "\n",
        "    for i, pred in enumerate(out):\n",
        "        uni_channels = torch.argmax(pred, dim=0).unique()\n",
        "        pred = pred.detach().cpu().numpy()\n",
        "        ax[i][0].imshow(x[i].squeeze(), cmap=\"gray\")\n",
        "        yi = y[i].squeeze()\n",
        "        ax[i][1].imshow(yi)\n",
        "\n",
        "        # Merge predicted masks into one image\n",
        "        mask = np.where(pred[uni_channels[0].item(),:,:] > 0.5, uni_channels[0].item(), 0)\n",
        "        for channel in uni_channels[1:]:\n",
        "            channel = channel.item()\n",
        "            channel_mask = np.where(pred[channel,:,:] > 0.5, channel, 0)\n",
        "            mask = mask | channel_mask\n",
        "        ax[i][2].imshow(mask)\n",
        "\n",
        "    ax[0][0].set_title(\"Image\")\n",
        "    ax[0][1].set_title(\"Ground Truth Mask\")\n",
        "    ax[0][2].set_title(\"Predicted Mask\")\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    multi_jaccard = MulticlassJaccardIndex(num_classes=num_classes, average=\"none\").cuda()\n",
        "    multi_jaccard.__name__ = \"iou\"\n",
        "    metrics = [multi_jaccard]\n",
        "\n",
        "    results = torch.zeros((1, num_classes))\n",
        "    for x, y in test_loader:\n",
        "        results += multi_jaccard(torch.softmax(model(x.cuda()), dim=1), y.cuda()).detach().cpu()\n",
        "\n",
        "    results = results/len(test_loader)\n",
        "\n",
        "    for i in range(0, len(class_labels)):\n",
        "        print(f\"{class_labels[i]} (Class {i}): {results[0][i]}\")\n",
        "\n",
        "    print(f\"Mean Testing IoU: {multi_jaccard(torch.softmax(model(x.cuda()), dim=1), y.cuda()).mean()}\")\n",
        "    return results"
      ],
      "metadata": {
        "id": "RNCMcoGoycz9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmentations = A.Compose([A.Resize(256, 256), ToTensorV2()])\n",
        "\n",
        "num_classes = 9"
      ],
      "metadata": {
        "id": "hGkOhTeavO0l"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = BonyAnatomyJointSegmentationDataset(data_save_location, train_all.id, num_classes,\n",
        "                                                transforms=augmentations)\n",
        "\n",
        "valid_set = BonyAnatomyJointSegmentationDataset(data_save_location, valid_all.id, num_classes,\n",
        "                                                transforms=augmentations)\n",
        "\n",
        "test_set = BonyAnatomyJointSegmentationDataset(data_save_location,test_all.id, num_classes,\n",
        "                                               transforms=augmentations)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=16, shuffle=True, num_workers=2)\n",
        "valid_loader = DataLoader(valid_set, batch_size=16, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_set, batch_size=5, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "MQExnRzVu0oh"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_model = train_model(train_loader, valid_loader, num_classes)"
      ],
      "metadata": {
        "id": "4xu2HCYrvLFm",
        "outputId": "24cc08ce-563c-459c-dac9-f8ccd255bcbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 382MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "train:   0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydicom/charset.py:743: UserWarning: Incorrect value for Specific Character Set 'ISO_2022_IR_6' - assuming 'ISO 2022 IR 6'\n",
            "  _warn_about_invalid_encoding(encoding, patched)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: 100%|██████████| 5/5 [00:14<00:00,  2.87s/it,  loss - 1.82, iou_score - 0.08472]\n",
            "valid: 100%|██████████| 1/1 [00:02<00:00,  2.60s/it,  loss - 1.937, iou_score - 0.04938]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 2\n",
            "train:  40%|████      | 2/5 [00:05<00:07,  2.45s/it,  loss - 1.482, iou_score - 0.1642]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydicom/charset.py:743: UserWarning: Incorrect value for Specific Character Set 'ISO_2022_IR_6' - assuming 'ISO 2022 IR 6'\n",
            "  _warn_about_invalid_encoding(encoding, patched)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: 100%|██████████| 5/5 [00:13<00:00,  2.63s/it,  loss - 1.384, iou_score - 0.2072]\n",
            "valid: 100%|██████████| 1/1 [00:03<00:00,  3.29s/it,  loss - 1.624, iou_score - 0.1125]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 3\n",
            "train:  40%|████      | 2/5 [00:04<00:06,  2.25s/it,  loss - 1.166, iou_score - 0.3249]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydicom/charset.py:743: UserWarning: Incorrect value for Specific Character Set 'ISO_2022_IR_6' - assuming 'ISO 2022 IR 6'\n",
            "  _warn_about_invalid_encoding(encoding, patched)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: 100%|██████████| 5/5 [00:12<00:00,  2.48s/it,  loss - 1.105, iou_score - 0.3489]\n",
            "valid: 100%|██████████| 1/1 [00:02<00:00,  2.52s/it,  loss - 1.114, iou_score - 0.1863]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 4\n",
            "train:   0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydicom/charset.py:743: UserWarning: Incorrect value for Specific Character Set 'ISO_2022_IR_6' - assuming 'ISO 2022 IR 6'\n",
            "  _warn_about_invalid_encoding(encoding, patched)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: 100%|██████████| 5/5 [00:11<00:00,  2.30s/it,  loss - 0.8944, iou_score - 0.4223]\n",
            "valid: 100%|██████████| 1/1 [00:02<00:00,  2.70s/it,  loss - 0.8631, iou_score - 0.3151]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 5\n",
            "train:   0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydicom/charset.py:743: UserWarning: Incorrect value for Specific Character Set 'ISO_2022_IR_6' - assuming 'ISO 2022 IR 6'\n",
            "  _warn_about_invalid_encoding(encoding, patched)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: 100%|██████████| 5/5 [00:10<00:00,  2.08s/it,  loss - 0.7349, iou_score - 0.4666]\n",
            "valid: 100%|██████████| 1/1 [00:03<00:00,  3.07s/it,  loss - 0.7185, iou_score - 0.4141]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 6\n",
            "train:  40%|████      | 2/5 [00:05<00:06,  2.23s/it,  loss - 0.6407, iou_score - 0.5209]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydicom/charset.py:743: UserWarning: Incorrect value for Specific Character Set 'ISO_2022_IR_6' - assuming 'ISO 2022 IR 6'\n",
            "  _warn_about_invalid_encoding(encoding, patched)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: 100%|██████████| 5/5 [00:09<00:00,  1.96s/it,  loss - 0.614, iou_score - 0.5275]\n",
            "valid: 100%|██████████| 1/1 [00:03<00:00,  3.06s/it,  loss - 0.6119, iou_score - 0.5288]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 7\n",
            "train:  40%|████      | 2/5 [00:08<00:13,  4.42s/it,  loss - 0.545, iou_score - 0.5499]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-7c303b51bdf2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbaseline_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-f70a8440e878>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_loader, valid_loader, num_classes)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nEpoch: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mtrain_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mvalid_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_epoch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/segmentation_models_pytorch/utils/train.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, dataloader)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         ) as iterator:\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1330\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1293\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_results = test_model(baseline_model, test_loader, classes)"
      ],
      "metadata": {
        "id": "XNJqSHp8xM5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gender Specific Baselines\n",
        "\n"
      ],
      "metadata": {
        "id": "-WhVqAY60fmQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Male"
      ],
      "metadata": {
        "id": "T-jqMABZ01F4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = BonyAnatomyJointSegmentationDataset(data_save_location, train_male.id, num_classes,\n",
        "                                                transforms=augmentations)\n",
        "\n",
        "valid_set = BonyAnatomyJointSegmentationDataset(data_save_location, valid_male.id, num_classes,\n",
        "                                                transforms=augmentations)\n",
        "\n",
        "test_set = BonyAnatomyJointSegmentationDataset(data_save_location, test_male.id, num_classes,\n",
        "                                               transforms=augmentations)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=16, shuffle=True, num_workers=2)\n",
        "valid_loader = DataLoader(valid_set, batch_size=16, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_set, batch_size=5, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "zIHKRcCNy3Xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_male_model = train_model(train_loader, valid_loader, num_classes)"
      ],
      "metadata": {
        "id": "6dggejIa0uuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_male_results = test_model(baseline_male_model, test_loader, classes)"
      ],
      "metadata": {
        "id": "Pz6FoGG90xJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Female"
      ],
      "metadata": {
        "id": "Qkz8__yN0xmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = BonyAnatomyJointSegmentationDataset(data_save_location, train_female.id, num_classes,\n",
        "                                                transforms=augmentations)\n",
        "\n",
        "valid_set = BonyAnatomyJointSegmentationDataset(data_save_location, valid_female.id, num_classes,\n",
        "                                                transforms=augmentations)\n",
        "\n",
        "test_set = BonyAnatomyJointSegmentationDataset(data_save_location, test_female.id, num_classes,\n",
        "                                               transforms=augmentations)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=16, shuffle=True, num_workers=2)\n",
        "valid_loader = DataLoader(valid_set, batch_size=16, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_set, batch_size=5, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "D0vm6ojw0yXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_female_model = train_model(train_loader, valid_loader, num_classes)"
      ],
      "metadata": {
        "id": "s2ixgt2b07tH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_female_results = test_model(baseline_female_model, test_loader, classes)"
      ],
      "metadata": {
        "id": "Wk-QFFOw086q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Race Specific Baselines"
      ],
      "metadata": {
        "id": "LcQL6vUu1C3z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### White/Caucasian"
      ],
      "metadata": {
        "id": "V09lqyWf1DT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = BonyAnatomyJointSegmentationDataset(data_save_location, train_white.id, num_classes,\n",
        "                                                transforms=augmentations)\n",
        "\n",
        "valid_set = BonyAnatomyJointSegmentationDataset(data_save_location, valid_white.id, num_classes,\n",
        "                                                transforms=augmentations)\n",
        "\n",
        "test_set = BonyAnatomyJointSegmentationDataset(data_save_location, test_white.id, num_classes,\n",
        "                                               transforms=augmentations)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=16, shuffle=True, num_workers=2)\n",
        "valid_loader = DataLoader(valid_set, batch_size=16, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_set, batch_size=5, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "z887u24L1JMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_white_model = train_model(train_loader, valid_loader, num_classes)"
      ],
      "metadata": {
        "id": "bsxG9HiF1QwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_white_results = test_model(baseline_white_model, test_loader, classes)"
      ],
      "metadata": {
        "id": "pgR51Z971S-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Black/African American (AA)"
      ],
      "metadata": {
        "id": "OOK8OSj11Vu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = BonyAnatomyJointSegmentationDataset(data_save_location, train_black.id, num_classes,\n",
        "                                                transforms=augmentations)\n",
        "\n",
        "valid_set = BonyAnatomyJointSegmentationDataset(data_save_location, valid_black.id, num_classes,\n",
        "                                                transforms=augmentations)\n",
        "\n",
        "test_set = BonyAnatomyJointSegmentationDataset(data_save_location, test_black.id, num_classes,\n",
        "                                               transforms=augmentations)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=16, shuffle=True, num_workers=2)\n",
        "valid_loader = DataLoader(valid_set, batch_size=16, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_set, batch_size=5, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "-QSA1noA1Y3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_black_model = train_model(train_loader, valid_loader, num_classes)"
      ],
      "metadata": {
        "id": "L-I6xBZU1ig-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_black_results = test_model(baseline_black_model, test_loader, classes)"
      ],
      "metadata": {
        "id": "h_gFVgU41iZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Balanced Models"
      ],
      "metadata": {
        "id": "WmQ4A8Qb1mM7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gender"
      ],
      "metadata": {
        "id": "KQqJShEY1t61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = BonyAnatomyJointSegmentationDataset(data_save_location, balanced_gender_train.id, num_classes,\n",
        "                                                transforms=augmentations)\n",
        "\n",
        "valid_set = BonyAnatomyJointSegmentationDataset(data_save_location, valid_all.id, num_classes,\n",
        "                                                transforms=augmentations)\n",
        "\n",
        "test_set = BonyAnatomyJointSegmentationDataset(data_save_location, test_all.id, num_classes,\n",
        "                                               transforms=augmentations)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=16, shuffle=True, num_workers=2)\n",
        "valid_loader = DataLoader(valid_set, batch_size=16, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_set, batch_size=5, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "7Ga9bXQW1su1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_gender_model = train_model(train_loader, valid_loader, num_classes)"
      ],
      "metadata": {
        "id": "pQ7qvroD2G42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_gender_results = test_model(balanced_gender_model, test_loader, classes)"
      ],
      "metadata": {
        "id": "WUMufBhS2K5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Race"
      ],
      "metadata": {
        "id": "tB8QGCGp2NVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = BonyAnatomyJointSegmentationDataset(data_save_location, balanced_race_train.id, num_classes,\n",
        "                                                transforms=augmentations)\n",
        "\n",
        "valid_set = BonyAnatomyJointSegmentationDataset(data_save_location, valid_all.id, num_classes,\n",
        "                                                transforms=augmentations)\n",
        "\n",
        "test_set = BonyAnatomyJointSegmentationDataset(data_save_location, test_all.id, num_classes,\n",
        "                                               transforms=augmentations)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=16, shuffle=True, num_workers=2)\n",
        "valid_loader = DataLoader(valid_set, batch_size=16, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_set, batch_size=5, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "XS4EVpNL2OUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_race_model = train_model(train_loader, valid_loader, num_classes)"
      ],
      "metadata": {
        "id": "0U_inJpS2Qn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_race_results = test_model(balanced_race_model, test_loader, classes)"
      ],
      "metadata": {
        "id": "s7hT7Ue62ScL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stratified Models"
      ],
      "metadata": {
        "id": "qYzD99KV2Tlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "class StratifiedSampler:\n",
        "    \"\"\"\n",
        "    Based on this Pytorch discussion board post\n",
        "    https://discuss.pytorch.org/t/how-to-enable-the-dataloader-to-sample-from-each-class-with-equal-probability/911/6\n",
        "    \"\"\"\n",
        "    def __init__(self, stratify_on, batch_size):\n",
        "        self.stratify_on = stratify_on\n",
        "        self.batch_size = batch_size\n",
        "        self.nsplits = int(len(stratify_on) / batch_size)\n",
        "\n",
        "    def gen_stratified_sample(self):\n",
        "        s = StratifiedKFold(n_splits = self.nsplits)\n",
        "\n",
        "        X = np.arange(0, len(self.stratify_on))\n",
        "        s.get_n_splits(X, self.stratify_on)\n",
        "        for train_idx, valid_idx in s.split(X, self.stratify_on):\n",
        "            yield valid_idx\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(self.gen_stratified_sample())\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.nsplits\n"
      ],
      "metadata": {
        "id": "gqcwHfWU2fIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gender"
      ],
      "metadata": {
        "id": "qfO8Zcmn2V6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = BonyAnatomyJointSegmentationDataset(data_save_location, train_all.id, num_classes,\n",
        "                                                transforms=augmentations)\n",
        "\n",
        "valid_set = BonyAnatomyJointSegmentationDataset(data_save_location, valid_all.id, num_classes,\n",
        "                                                transforms=augmentations)\n",
        "\n",
        "test_set = BonyAnatomyJointSegmentationDataset(data_save_location,test_all.id, num_classes,\n",
        "                                               transforms=augmentations)\n",
        "\n",
        "train_loader = DataLoader(train_set, num_workers=2, batch_sampler=StratifiedSampler(train_all.P02SEX, batch_size=16))\n",
        "valid_loader = DataLoader(valid_set, batch_size=16, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_set, batch_size=5, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "QcXW9oX-2U3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stratified_gender_model = train_model(train_loader, valid_loader, num_classes)"
      ],
      "metadata": {
        "id": "_iUzuAmK27ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stratified_gender_results = test_model(stratified_gender_model, test_loader, classes)"
      ],
      "metadata": {
        "id": "rkw-yitS2-iG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Race"
      ],
      "metadata": {
        "id": "APvSw5Te3BJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = BonyAnatomyJointSegmentationDataset(data_save_location, train_all.id, num_classes,\n",
        "                                                transforms=augmentations)\n",
        "\n",
        "valid_set = BonyAnatomyJointSegmentationDataset(data_save_location, valid_all.id, num_classes,\n",
        "                                                transforms=augmentations)\n",
        "\n",
        "test_set = BonyAnatomyJointSegmentationDataset(data_save_location,test_all.id, num_classes,\n",
        "                                               transforms=augmentations)\n",
        "\n",
        "train_loader = DataLoader(train_set, num_workers=2, batch_sampler=StratifiedSampler(train_all.P02RACE, batch_size=16))\n",
        "valid_loader = DataLoader(valid_set, batch_size=16, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_set, batch_size=5, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "pNhGw92A3CDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stratified_race_model = train_model(train_loader, valid_loader, num_classes)"
      ],
      "metadata": {
        "id": "0V-vHw8B3Eq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stratified_race_results = test_model(stratified_race_model, test_loader, classes)"
      ],
      "metadata": {
        "id": "wnQhEyzv3H1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Bias"
      ],
      "metadata": {
        "id": "7wLJyTBiIUcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_bias(model, dataset, metric, num_classes = 9):\n",
        "    results = torch.zeros((1, num_classes))\n",
        "    test_loader = generate_dataloader(dataset, num_classes)\n",
        "    for x, y in test_loader:\n",
        "        results += metric(torch.softmax(model(x.cuda()), dim=1), y.cuda()).detach().cpu()\n",
        "\n",
        "    return results/len(test_loader)\n",
        "\n",
        "def generate_dataloader(dataset, num_classes):\n",
        "    augmentations = A.Compose([A.Resize(256, 256), ToTensorV2()]) # A.OneOf([A.Emboss(), Canny()\n",
        "    test_set = BonyAnatomyJointSegmentationDataset(data_save_location, dataset.id, num_classes,\n",
        "                                                   transforms=augmentations)\n",
        "    test_loader = DataLoader(test_set, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "    return test_loader"
      ],
      "metadata": {
        "id": "XW7id1fHI3n2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multi_jaccard = MulticlassJaccardIndex(num_classes=9, average=\"none\").cuda()\n",
        "multi_jaccard.__name__ = \"iou\""
      ],
      "metadata": {
        "id": "S9tiKIDKJBeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_records = pd.read_csv(data_location)"
      ],
      "metadata": {
        "id": "PtSiyDvEIjxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "male_group = data_records[data_records.P02SEX == \"1: Male\"].reset_index(drop=True)\n",
        "female_group = data_records[data_records.P02SEX == \"2: Female\"].reset_index(drop=True)\n",
        "white_caucasian_group = data_records[data_records.P02RACE == \"1: White or Caucasian\"].reset_index(drop=True)\n",
        "black_aa_group = data_records[data_records.P02RACE == \"2: Black or African American\"].reset_index(drop=True)"
      ],
      "metadata": {
        "id": "5dViCGL1IWOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gender_male_metrics_baseline = eval_bias(baseline_model, male_group, multi_jaccard)\n",
        "gender_female_metrics_baseline = eval_bias(baseline_model, female_group, multi_jaccard)\n",
        "race_white_metrics_baseline = eval_bias(baseline_model, white_caucasian_group, multi_jaccard)\n",
        "race_black_metrics_baseline = eval_bias(baseline_model, black_aa_group, multi_jaccard)\n",
        "\n",
        "base_dict = {\n",
        "    \"Gender: Male\": gender_male_metrics_baseline.numpy().squeeze(),\n",
        "    \"Gender: Female\": gender_female_metrics_baseline.numpy().squeeze(),\n",
        "    \"Race: White/Caucasian\": race_white_metrics_baseline.numpy().squeeze(),\n",
        "    \"Race: Black/AA\": race_black_metrics_baseline.numpy().squeeze(),\n",
        "}\n",
        "\n",
        "baseline_res = pd.DataFrame(base_dict).T\n",
        "baseline_res[\"Average\"] = baseline_res.mean(axis=1)\n",
        "baseline_res.columns = [val for key, val in classes.items()] + [\"Average\"]\n",
        "baseline_res"
      ],
      "metadata": {
        "id": "B6-6QFQlIwQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gender_male_metrics_balanced = eval_bias(balanced_gender_model, male_group, multi_jaccard)\n",
        "gender_female_metrics_balanced = eval_bias(balanced_gender_model, female_group, multi_jaccard)\n",
        "\n",
        "balanced_gender = {\n",
        "    \"Gender: Male\": gender_male_metrics_balanced.numpy().squeeze(),\n",
        "    \"Gender: Female\": gender_female_metrics_balanced.numpy().squeeze(),\n",
        "}\n",
        "balanced_gender_res = pd.DataFrame(balanced_gender).T\n",
        "balanced_gender_res[\"Average\"] =  balanced_gender_res.mean(axis=1)\n",
        "balanced_gender_res.columns = [val for key, val in classes.items()] + [\"Average\"]\n",
        "\n",
        "balanced_gender_res"
      ],
      "metadata": {
        "id": "b0N3EhkmKFjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "race_white_metrics_balanced = eval_bias(balanced_race_model, white_caucasian_group, multi_jaccard)\n",
        "race_black_metrics_balanced = eval_bias(balanced_race_model, black_aa_group, multi_jaccard)\n",
        "\n",
        "balanced_race = {\n",
        "     \"Race: White/Caucasian\": race_white_metrics_balanced.numpy().squeeze(),\n",
        "    \"Race: Black/AA\": race_black_metrics_balanced.numpy().squeeze(),\n",
        "}\n",
        "balanced_race_res = pd.DataFrame(balanced_race).T\n",
        "balanced_race_res[\"Average\"] = balanced_race_res.mean(axis=1)\n",
        "balanced_race_res.columns = [val for key, val in classes.items()] + [\"Average\"]\n",
        "\n",
        "balanced_race_res"
      ],
      "metadata": {
        "id": "Xm6YQ9RkKV5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gender_male_metrics_stratified = eval_bias(stratified_gender_model, male_group, multi_jaccard)\n",
        "gender_female_metrics_stratified = eval_bias(stratified_gender_model, female_group, multi_jaccard)\n",
        "\n",
        "strat_gender = {\n",
        "    \"Gender: Male\": gender_male_metrics_stratified.numpy().squeeze(),\n",
        "    \"Gender: Female\": gender_female_metrics_stratified.numpy().squeeze(),\n",
        "}\n",
        "strat_gender_res = pd.DataFrame(strat_gender).T\n",
        "strat_gender_res[\"Average\"] = strat_gender_res.mean(axis=1)\n",
        "strat_gender_res.columns = [val for key, val in classes.items()] + [\"Average\"]\n",
        "strat_gender_res"
      ],
      "metadata": {
        "id": "UY0j5j6zK_yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "race_white_metrics_stratified = eval_bias(stratified_race_model, white_caucasian_group, multi_jaccard)\n",
        "race_black_metrics_stratified= eval_bias(stratified_race_model, black_aa_group, multi_jaccard)\n",
        "\n",
        "strat_race = {\n",
        "    \"Race: White/Caucasian\": race_white_metrics_stratified.numpy().squeeze(),\n",
        "    \"Race: Black/AA\": race_black_metrics_stratified.numpy().squeeze(),\n",
        "}\n",
        "strat_race_res = pd.DataFrame(strat_race).T\n",
        "strat_race_res[\"Average\"] = strat_race_res.mean(axis=1)\n",
        "strat_race_res.columns = [val for key, val in classes.items()] + [\"Average\"]\n",
        "strat_race_res"
      ],
      "metadata": {
        "id": "Xd-7qEYTK3EI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gender_male_metrics = eval_bias(baseline_male_model, male_group, multi_jaccard)\n",
        "\n",
        "gender_male = {\n",
        "    \"Gender: Male\": gender_male_metrics.numpy().squeeze(),\n",
        "}\n",
        "gender_male = pd.DataFrame(gender_male).T\n",
        "gender_male[\"Average\"] = gender_male.mean(axis=1)\n",
        "gender_male.columns = [val for key, val in classes.items()] + [\"Average\"]\n",
        "gender_male"
      ],
      "metadata": {
        "id": "6X_5WxHmLKTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gender_female_metrics = eval_bias(baseline_female_model, female_group, multi_jaccard)\n",
        "\n",
        "gender_female = {\n",
        "    \"Gender: Female\": gender_female_metrics.numpy().squeeze(),\n",
        "}\n",
        "gender_female = pd.DataFrame(gender_female).T\n",
        "gender_female[\"Average\"] = gender_female.mean(axis=1)\n",
        "gender_female.columns = [val for key, val in classes.items()] + [\"Average\"]\n",
        "gender_female"
      ],
      "metadata": {
        "id": "8wxnEBqRLUyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "race_white_metrics = eval_bias(baseline_white_model, white_caucasian_group, multi_jaccard)\n",
        "\n",
        "race_white = {\n",
        "    \"Race: White/Caucasian\": race_white_metrics.numpy().squeeze(),\n",
        "}\n",
        "race_white = pd.DataFrame(race_white).T\n",
        "race_white[\"Average\"] = race_white.mean(axis=1)\n",
        "race_white.columns = [val for key, val in classes.items()] + [\"Average\"]\n",
        "race_white"
      ],
      "metadata": {
        "id": "Lf31YIEZLcEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "race_black_metrics = eval_bias(baseline_black_model, black_aa_group, multi_jaccard)\n",
        "\n",
        "race_black = {\n",
        "    \"Race: Black/AA\": race_black_metrics.numpy().squeeze(),\n",
        "}\n",
        "race_black = pd.DataFrame(race_black).T\n",
        "race_black[\"Average\"] = race_black.mean(axis=1)\n",
        "race_black.columns = [val for key, val in classes.items()] + [\"Average\"]\n",
        "race_black"
      ],
      "metadata": {
        "id": "pNeHkyERLpNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_fairness_score(data):\n",
        "    sd = []\n",
        "    ser = []\n",
        "\n",
        "    for i, row in data.iterrows():\n",
        "        sd.append(row[1:].std())\n",
        "        min_group = row[1:].min()\n",
        "        max_group = row[1:].max()\n",
        "        ser.append((1 - min_group)/(1 - max_group))\n",
        "\n",
        "    return sd, ser"
      ],
      "metadata": {
        "id": "R_DvIH88L5WB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fairness_gender = {\n",
        "    \"Model\": [\"Baseline\", \"Balanced\", \"Stratified\", \"Group-Specific\"],\n",
        "    \"Male\": [baseline_res.loc[\"Gender: Male\", \"Average\"],\n",
        "             balanced_gender_res.loc[\"Gender: Male\", \"Average\"],\n",
        "             strat_gender_res.loc[\"Gender: Male\", \"Average\"],\n",
        "             gender_male.loc[\"Gender: Male\", \"Average\"]],\n",
        "    \"Female\":[baseline_res.loc[\"Gender: Female\", \"Average\"],\n",
        "             balanced_gender_res.loc[\"Gender: Female\", \"Average\"],\n",
        "             strat_gender_res.loc[\"Gender: Female\", \"Average\"],\n",
        "             gender_female.loc[\"Gender: Female\", \"Average\"]]\n",
        "}\n",
        "\n",
        "fairness_df_gender = pd.DataFrame(fairness_gender)\n",
        "sd, ser = calc_fairness_score(fairness_df_gender)\n",
        "fairness_df_gender[\"SD\"] = sd\n",
        "fairness_df_gender[\"SER\"] = ser\n",
        "\n",
        "fairness_df_gender"
      ],
      "metadata": {
        "id": "ozSgh74AMA20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fairness_race = {\n",
        "    \"Model\": [\"Baseline\", \"Balanced\", \"Stratified\", \"Group-Specific\"],\n",
        "    \"White/Caucasian\": [baseline_res.loc[\"Race: White/Caucasian\", \"Average\"],\n",
        "             balanced_race_res.loc[\"Race: White/Caucasian\", \"Average\"],\n",
        "             strat_race_res.loc[\"Race: White/Caucasian\", \"Average\"],\n",
        "             race_white.loc[\"Race: White/Caucasian\", \"Average\"]],\n",
        "    \"Black/African American\":[baseline_res.loc[\"Race: Black/AA\", \"Average\"],\n",
        "             balanced_race_res.loc[\"Race: Black/AA\", \"Average\"],\n",
        "             strat_race_res.loc[\"Race: Black/AA\", \"Average\"],\n",
        "             race_black.loc[\"Race: Black/AA\", \"Average\"]]\n",
        "}\n",
        "\n",
        "fairness_df_race = pd.DataFrame(fairness_race)\n",
        "sd, ser = calc_fairness_score(fairness_df_race)\n",
        "fairness_df_race[\"SD\"] = sd\n",
        "fairness_df_race[\"SER\"] = ser\n",
        "\n",
        "fairness_df_race"
      ],
      "metadata": {
        "id": "PFAyOnhAMEN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ANz0PcXYNmsB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}