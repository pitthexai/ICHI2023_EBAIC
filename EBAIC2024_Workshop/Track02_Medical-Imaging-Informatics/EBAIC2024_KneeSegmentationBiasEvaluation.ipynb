{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOTya4W+rqKcKmW9690wDUD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pitthexai/ICHI2023_EBAIC/blob/main/EBAIC2024_Workshop/Track02_Medical-Imaging-Informatics/EBAIC2024_KneeSegmentationBiasEvaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EBAIC 2024 Track II: Medical Imaging Informatics: Fair Knee Anatomy Segmentation"
      ],
      "metadata": {
        "id": "c9FKzHsdt53U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Package Setup"
      ],
      "metadata": {
        "id": "JcB5b0PHRgW8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s93SFhrOrxTa"
      },
      "outputs": [],
      "source": [
        "!pip install segmentation-models-pytorch albumentations torchmetrics pydicom nibabel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "import copy\n",
        "\n",
        "from zipfile import ZipFile\n",
        "\n",
        "from io import BytesIO\n",
        "from gzip import GzipFile\n",
        "\n",
        "import os\n",
        "\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "import pydicom\n",
        "import nibabel\n",
        "from nibabel import FileHolder, Nifti1Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "import segmentation_models_pytorch as smp\n",
        "from segmentation_models_pytorch import utils as smp_utils\n",
        "\n",
        "from torchmetrics.classification import MulticlassJaccardIndex, Dice\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Pd3G4QOJs0KE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "yevZb-zWr5HB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_STATE = 42"
      ],
      "metadata": {
        "id": "Hqi8DEP2sI34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Data Preprocessing"
      ],
      "metadata": {
        "id": "8rjWsBFbt2Vh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directories for\n",
        "\n",
        "filename = \"knee_seg_sample.csv\"\n",
        "directory = \"KneeSample\"\n",
        "zipfile = \"knee_sample.zip\"\n",
        "\n",
        "classes = {\n",
        "    0: \"Background\",\n",
        "    1: \"R Patella\",\n",
        "    2: \"R Femur\",\n",
        "    3: \"R Tibia\",\n",
        "    4: \"R Fibula\",\n",
        "    5: \"L Patella\",\n",
        "    6: \"L Femur\",\n",
        "    7: \"L Tibia\",\n",
        "    8: \"L Fibula\"\n",
        "}\n",
        "\n",
        "zipfile_loc = f\"/content/drive/MyDrive/GoogleColabProjects/EBAIC_2024/KneeDataset/{zipfile}\"\n",
        "data_location = f\"/content/data/{directory}/{filename}\"\n",
        "data_save_location = f\"/content/data/{directory}/\""
      ],
      "metadata": {
        "id": "bc8qVtmMsRB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M9YCLP7atBq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(data_save_location):\n",
        "    with ZipFile(zipfile_loc, 'r') as zipf:\n",
        "        zipf.extractall(\"/content/\")"
      ],
      "metadata": {
        "id": "p3ZWmq51susS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_train_test_split(csv_pth, filter_query=None):\n",
        "    data_records = pd.read_csv(csv_pth)\n",
        "    data_records = data_records[data_records.id != 9025994].reset_index(drop=True)\n",
        "    if filter_query:\n",
        "        data_records = data_records.query(filter_query)\n",
        "\n",
        "    train, test = train_test_split(data_records.id.unique(), test_size=0.3, random_state=42)\n",
        "    valid, test = train_test_split(test, test_size=0.5, random_state=42)\n",
        "\n",
        "    train = data_records[data_records.id.isin(train)].reset_index(drop=True)\n",
        "    valid = data_records[data_records.id.isin(valid)].reset_index(drop=True)\n",
        "    test = data_records[data_records.id.isin(test)].reset_index(drop=True)\n",
        "\n",
        "    return train, valid, test"
      ],
      "metadata": {
        "id": "e2Qwhgqysxrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def balance_dataset(data, by_filter1, by_filter2):\n",
        "    filtered1 = data.query(by_filter1)\n",
        "    filtered2 = data.query(by_filter2)\n",
        "\n",
        "    min_sample_size = np.minimum(len(filtered1), len(filtered2))\n",
        "    samp1 = filtered1.sample(min_sample_size,  random_state = 42)\n",
        "    samp2 = filtered2.sample(min_sample_size,  random_state = 42)\n",
        "\n",
        "    balanced_data = pd.concat([samp1, samp2]).reset_index(drop=True)\n",
        "\n",
        "    print(f\"Training dataset reduced from size of {len(data)} samples to a balanced dataset of size {len(balanced_data)} samples\")\n",
        "\n",
        "    return balanced_data"
      ],
      "metadata": {
        "id": "eagrBr6TtHxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Baseline Datasets\n",
        "train_all, valid_all, test_all = generate_train_test_split(data_location)\n",
        "train_white, valid_white, test_white = generate_train_test_split(data_location, filter_query=\"P02RACE == '1: White or Caucasian'\")\n",
        "train_black, valid_black, test_black = generate_train_test_split(data_location, filter_query=\"P02RACE == '2: Black or African American'\")\n",
        "train_male, valid_male, test_male = generate_train_test_split(data_location, filter_query=\"P02SEX == '1: Male'\")\n",
        "train_female, valid_female, test_female = generate_train_test_split(data_location, filter_query=\"P02SEX == '2: Female'\")\n",
        "\n",
        "balanced_gender_train = balance_dataset(train_all, \"P02SEX == '1: Male'\", \"P02SEX == '2: Female'\")\n",
        "balanced_race_train = balance_dataset(train_all, \"P02RACE == '1: White or Caucasian'\", \"P02RACE == '2: Black or African American'\")"
      ],
      "metadata": {
        "id": "G1e695BotJZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Modeling"
      ],
      "metadata": {
        "id": "e5F0R7TyR3kM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Baseline Models\n"
      ],
      "metadata": {
        "id": "ZFxmkKsot_9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BonyAnatomyJointSegmentationDataset(Dataset):\n",
        "    def __init__(self, root_dir, ids, num_classes, transforms=None, preprocessing=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.pids = ids\n",
        "        self.num_classes = num_classes\n",
        "        self.transforms = transforms\n",
        "        self.preprocessing = preprocessing\n",
        "\n",
        "    def load_dicom(self, path):\n",
        "        dicom_img = pydicom.dcmread(path)\n",
        "        return dicom_img.pixel_array.astype(np.float32)\n",
        "\n",
        "    def load_nii(self, path):\n",
        "        nii_annot = nibabel.load(path)\n",
        "        nii_annot_data = nii_annot.get_fdata()\n",
        "\n",
        "        if len(nii_annot_data.shape) == 3 and nii_annot_data.shape[-1] > 1:\n",
        "            if nii_annot_data.shape[-1] == 2:\n",
        "                nii_annot_data = nii_annot_data[:, :, 1]\n",
        "            else:\n",
        "                nii_annot_data = nii_annot_data[:, :, nii_annot_data.shape[-1]//2]\n",
        "\n",
        "            nii_annot_data = np.expand_dims(nii_annot_data, axis=-1)\n",
        "\n",
        "\n",
        "        nii_annot_data = cv2.rotate(nii_annot_data, cv2.ROTATE_90_CLOCKWISE)\n",
        "        nii_annot_data = cv2.flip(nii_annot_data, 1)\n",
        "        return nii_annot_data\n",
        "\n",
        "    def get_file_path(self, filename):\n",
        "        return os.path.join(self.root_dir, filename)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        image = self.load_dicom(self.get_file_path(os.path.join(\"Images/\", str(self.pids[idx]) + \".dcm\")))\n",
        "        mask = self.load_nii(self.get_file_path(os.path.join(\"Annotations\", str(self.pids[idx]) + \".nii.gz\")))\n",
        "\n",
        "#         if len(np.unique(mask)) != self.num_classes:\n",
        "#             print(self.pids[idx])\n",
        "        if self.transforms is not None:\n",
        "            transformed = self.transforms(image=image, mask=mask)\n",
        "            image = transformed[\"image\"]\n",
        "            mask = transformed[\"mask\"]\n",
        "        if self.preprocessing is not None:\n",
        "            transformed = self.preprocessing(image=image, mask=mask)\n",
        "            image = transformed[\"image\"]\n",
        "            mask = transformed[\"mask\"]\n",
        "\n",
        "        return image.type(torch.FloatTensor), mask.long()\n"
      ],
      "metadata": {
        "id": "RoQs24RmuUKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(train_loader, valid_loader, num_classes=9):\n",
        "    encoder = \"resnet18\"\n",
        "    encoder_weights = \"imagenet\"\n",
        "    activation = None\n",
        "\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = copy.deepcopy(smp.Unet(encoder_name=encoder, encoder_weights=encoder_weights, in_channels=1,\n",
        "                    classes=num_classes, activation=activation)).to(device)\n",
        "    model.encoder.requires_grad_ = False\n",
        "    model.decoder.requires_grad_ = False\n",
        "\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "    loss.__name__=\" loss\"\n",
        "\n",
        "    multi_jaccard = MulticlassJaccardIndex(num_classes=num_classes, average=\"macro\").to(device)\n",
        "    multi_jaccard.__name__ = \"iou_score\"\n",
        "    metrics = [multi_jaccard]\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-04)\n",
        "\n",
        "    # create epoch runners\n",
        "    # it is a simple loop of iterating over dataloader`s samples\n",
        "    train_epoch = smp.utils.train.TrainEpoch(\n",
        "        model,\n",
        "        loss=loss,\n",
        "        metrics=metrics,\n",
        "        optimizer=optimizer,\n",
        "        device=device,\n",
        "        verbose=True,\n",
        "    )\n",
        "\n",
        "    valid_epoch = smp.utils.train.ValidEpoch(\n",
        "        model,\n",
        "        loss=loss,\n",
        "        metrics=metrics,\n",
        "        device=device,\n",
        "        verbose=True,\n",
        "    )\n",
        "\n",
        "    max_score = 0\n",
        "\n",
        "    for i in range(1, 51):\n",
        "\n",
        "        print('\\nEpoch: {}'.format(i))\n",
        "        train_logs = train_epoch.run(train_loader)\n",
        "        valid_logs = valid_epoch.run(valid_loader)\n",
        "\n",
        "        # do something (save model, change lr, etc.)\n",
        "        if max_score < valid_logs['iou_score']:\n",
        "            max_score = valid_logs['iou_score']\n",
        "            torch.save(model, './best_model.pth')\n",
        "            print('Model saved!')\n",
        "\n",
        "    # Return best model\n",
        "    model = torch.load('./best_model.pth')\n",
        "    return model"
      ],
      "metadata": {
        "id": "EvQi5plPvfu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, test_loader, class_labels):\n",
        "    model.eval()\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    fig, ax = plt.subplots(nrows=5, ncols=3, figsize=(10, 10), sharex=True, sharey=True)\n",
        "\n",
        "    x, y = next(iter(test_loader))\n",
        "    out = torch.softmax(model(x.to(device)), dim=1)\n",
        "    #out = out.detach().cpu().numpy()\n",
        "\n",
        "    for i, pred in enumerate(out):\n",
        "        uni_channels = torch.argmax(pred, dim=0).unique()\n",
        "        pred = pred.detach().cpu().numpy()\n",
        "        ax[i][0].imshow(x[i].squeeze(), cmap=\"gray\")\n",
        "        yi = y[i].squeeze()\n",
        "        ax[i][1].imshow(yi)\n",
        "\n",
        "        # Merge predicted masks into one image\n",
        "        mask = np.where(pred[uni_channels[0].item(),:,:] > 0.5, uni_channels[0].item(), 0)\n",
        "        for channel in uni_channels[1:]:\n",
        "            channel = channel.item()\n",
        "            channel_mask = np.where(pred[channel,:,:] > 0.5, channel, 0)\n",
        "            mask = mask | channel_mask\n",
        "        ax[i][2].imshow(mask)\n",
        "\n",
        "    ax[0][0].set_title(\"Image\")\n",
        "    ax[0][1].set_title(\"Ground Truth Mask\")\n",
        "    ax[0][2].set_title(\"Predicted Mask\")\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    multi_jaccard = MulticlassJaccardIndex(num_classes=num_classes, average=\"none\").cuda()\n",
        "    multi_jaccard.__name__ = \"iou\"\n",
        "    metrics = [multi_jaccard]\n",
        "\n",
        "    results = torch.zeros((1, num_classes))\n",
        "    for x, y in test_loader:\n",
        "        results += multi_jaccard(torch.softmax(model(x.cuda()), dim=1), y.cuda()).detach().cpu()\n",
        "\n",
        "    results = results/len(test_loader)\n",
        "\n",
        "    for i in range(0, len(class_labels)):\n",
        "        print(f\"{class_labels[i]} (Class {i}): {results[0][i]}\")\n",
        "\n",
        "    print(f\"Mean Testing IoU: {multi_jaccard(torch.softmax(model(x.cuda()), dim=1), y.cuda()).mean()}\")\n",
        "    return results"
      ],
      "metadata": {
        "id": "RNCMcoGoycz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmentations = A.Compose([A.Resize(256, 256), ToTensorV2()])\n",
        "\n",
        "num_classes = 9"
      ],
      "metadata": {
        "id": "hGkOhTeavO0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = BonyAnatomyJointSegmentationDataset(data_save_location, train_all.id, num_classes,\n",
        "                                                transforms=augmentations)\n",
        "\n",
        "valid_set = BonyAnatomyJointSegmentationDataset(data_save_location, valid_all.id, num_classes,\n",
        "                                                transforms=augmentations)\n",
        "\n",
        "test_set = BonyAnatomyJointSegmentationDataset(data_save_location,test_all.id, num_classes,\n",
        "                                               transforms=augmentations)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=16, shuffle=True, num_workers=2)\n",
        "valid_loader = DataLoader(valid_set, batch_size=16, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_set, batch_size=5, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "MQExnRzVu0oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_model = train_model(train_loader, valid_loader, num_classes)"
      ],
      "metadata": {
        "id": "4xu2HCYrvLFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_results = test_model(baseline_model, test_loader, classes)"
      ],
      "metadata": {
        "id": "XNJqSHp8xM5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gender Specific Baselines\n",
        "\n"
      ],
      "metadata": {
        "id": "-WhVqAY60fmQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Male"
      ],
      "metadata": {
        "id": "T-jqMABZ01F4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = BonyAnatomyJointSegmentationDataset(data_save_location, train_male.id, num_classes,\n",
        "                                                transforms=augmentations)\n",
        "\n",
        "valid_set = BonyAnatomyJointSegmentationDataset(data_save_location, valid_male.id, num_classes,\n",
        "                                                transforms=augmentations)\n",
        "\n",
        "test_set = BonyAnatomyJointSegmentationDataset(data_save_location, test_male.id, num_classes,\n",
        "                                               transforms=augmentations)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=16, shuffle=True, num_workers=2)\n",
        "valid_loader = DataLoader(valid_set, batch_size=16, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_set, batch_size=5, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "zIHKRcCNy3Xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_male_model = train_model(train_loader, valid_loader, num_classes)"
      ],
      "metadata": {
        "id": "6dggejIa0uuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_male_results = test_model(baseline_male_model, test_loader, classes)"
      ],
      "metadata": {
        "id": "Pz6FoGG90xJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Female"
      ],
      "metadata": {
        "id": "Qkz8__yN0xmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = BonyAnatomyJointSegmentationDataset(data_save_location, train_female.id, num_classes,\n",
        "                                                transforms=augmentations)\n",
        "\n",
        "valid_set = BonyAnatomyJointSegmentationDataset(data_save_location, valid_female.id, num_classes,\n",
        "                                                transforms=augmentations)\n",
        "\n",
        "test_set = BonyAnatomyJointSegmentationDataset(data_save_location, test_female.id, num_classes,\n",
        "                                               transforms=augmentations)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=16, shuffle=True, num_workers=2)\n",
        "valid_loader = DataLoader(valid_set, batch_size=16, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_set, batch_size=5, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "D0vm6ojw0yXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_female_model = train_model(train_loader, valid_loader, num_classes)"
      ],
      "metadata": {
        "id": "s2ixgt2b07tH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_female_results = test_model(baseline_female_model, test_loader, classes)"
      ],
      "metadata": {
        "id": "Wk-QFFOw086q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Race Specific Baselines"
      ],
      "metadata": {
        "id": "LcQL6vUu1C3z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### White/Caucasian"
      ],
      "metadata": {
        "id": "V09lqyWf1DT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = BonyAnatomyJointSegmentationDataset(data_save_location, train_white.id, num_classes,\n",
        "                                                transforms=augmentations)\n",
        "\n",
        "valid_set = BonyAnatomyJointSegmentationDataset(data_save_location, valid_white.id, num_classes,\n",
        "                                                transforms=augmentations)\n",
        "\n",
        "test_set = BonyAnatomyJointSegmentationDataset(data_save_location, test_white.id, num_classes,\n",
        "                                               transforms=augmentations)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=16, shuffle=True, num_workers=2)\n",
        "valid_loader = DataLoader(valid_set, batch_size=16, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_set, batch_size=5, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "z887u24L1JMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_white_model = train_model(train_loader, valid_loader, num_classes)"
      ],
      "metadata": {
        "id": "bsxG9HiF1QwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_white_results = test_model(baseline_white_model, test_loader, classes)"
      ],
      "metadata": {
        "id": "pgR51Z971S-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Black/African American (AA)"
      ],
      "metadata": {
        "id": "OOK8OSj11Vu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = BonyAnatomyJointSegmentationDataset(data_save_location, train_black.id, num_classes,\n",
        "                                                transforms=augmentations)\n",
        "\n",
        "valid_set = BonyAnatomyJointSegmentationDataset(data_save_location, valid_black.id, num_classes,\n",
        "                                                transforms=augmentations)\n",
        "\n",
        "test_set = BonyAnatomyJointSegmentationDataset(data_save_location, test_black.id, num_classes,\n",
        "                                               transforms=augmentations)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=16, shuffle=True, num_workers=2)\n",
        "valid_loader = DataLoader(valid_set, batch_size=16, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_set, batch_size=5, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "-QSA1noA1Y3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_black_model = train_model(train_loader, valid_loader, num_classes)"
      ],
      "metadata": {
        "id": "L-I6xBZU1ig-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_black_results = test_model(baseline_black_model, test_loader, classes)"
      ],
      "metadata": {
        "id": "h_gFVgU41iZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Balanced Models"
      ],
      "metadata": {
        "id": "WmQ4A8Qb1mM7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Gender"
      ],
      "metadata": {
        "id": "KQqJShEY1t61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = BonyAnatomyJointSegmentationDataset(data_save_location, balanced_gender_train.id, num_classes,\n",
        "                                                transforms=augmentations)\n",
        "\n",
        "valid_set = BonyAnatomyJointSegmentationDataset(data_save_location, valid_all.id, num_classes,\n",
        "                                                transforms=augmentations)\n",
        "\n",
        "test_set = BonyAnatomyJointSegmentationDataset(data_save_location, test_all.id, num_classes,\n",
        "                                               transforms=augmentations)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=16, shuffle=True, num_workers=2)\n",
        "valid_loader = DataLoader(valid_set, batch_size=16, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_set, batch_size=5, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "7Ga9bXQW1su1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_gender_model = train_model(train_loader, valid_loader, num_classes)"
      ],
      "metadata": {
        "id": "pQ7qvroD2G42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_gender_results = test_model(balanced_gender_model, test_loader, classes)"
      ],
      "metadata": {
        "id": "WUMufBhS2K5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Race"
      ],
      "metadata": {
        "id": "tB8QGCGp2NVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = BonyAnatomyJointSegmentationDataset(data_save_location, balanced_race_train.id, num_classes,\n",
        "                                                transforms=augmentations)\n",
        "\n",
        "valid_set = BonyAnatomyJointSegmentationDataset(data_save_location, valid_all.id, num_classes,\n",
        "                                                transforms=augmentations)\n",
        "\n",
        "test_set = BonyAnatomyJointSegmentationDataset(data_save_location, test_all.id, num_classes,\n",
        "                                               transforms=augmentations)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=16, shuffle=True, num_workers=2)\n",
        "valid_loader = DataLoader(valid_set, batch_size=16, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_set, batch_size=5, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "XS4EVpNL2OUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_race_model = train_model(train_loader, valid_loader, num_classes)"
      ],
      "metadata": {
        "id": "0U_inJpS2Qn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_race_results = test_model(balanced_race_model, test_loader, classes)"
      ],
      "metadata": {
        "id": "s7hT7Ue62ScL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stratified Models"
      ],
      "metadata": {
        "id": "qYzD99KV2Tlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "class StratifiedSampler:\n",
        "    \"\"\"\n",
        "    Based on this Pytorch discussion board post\n",
        "    https://discuss.pytorch.org/t/how-to-enable-the-dataloader-to-sample-from-each-class-with-equal-probability/911/6\n",
        "    \"\"\"\n",
        "    def __init__(self, stratify_on, batch_size):\n",
        "        self.stratify_on = stratify_on\n",
        "        self.batch_size = batch_size\n",
        "        self.nsplits = int(len(stratify_on) / batch_size)\n",
        "\n",
        "    def gen_stratified_sample(self):\n",
        "        s = StratifiedKFold(n_splits = self.nsplits)\n",
        "\n",
        "        X = np.arange(0, len(self.stratify_on))\n",
        "        s.get_n_splits(X, self.stratify_on)\n",
        "        for train_idx, valid_idx in s.split(X, self.stratify_on):\n",
        "            yield valid_idx\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(self.gen_stratified_sample())\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.nsplits\n"
      ],
      "metadata": {
        "id": "gqcwHfWU2fIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Gender"
      ],
      "metadata": {
        "id": "qfO8Zcmn2V6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = BonyAnatomyJointSegmentationDataset(data_save_location, train_all.id, num_classes,\n",
        "                                                transforms=augmentations)\n",
        "\n",
        "valid_set = BonyAnatomyJointSegmentationDataset(data_save_location, valid_all.id, num_classes,\n",
        "                                                transforms=augmentations)\n",
        "\n",
        "test_set = BonyAnatomyJointSegmentationDataset(data_save_location,test_all.id, num_classes,\n",
        "                                               transforms=augmentations)\n",
        "\n",
        "train_loader = DataLoader(train_set, num_workers=2, batch_sampler=StratifiedSampler(train_all.P02SEX, batch_size=16))\n",
        "valid_loader = DataLoader(valid_set, batch_size=16, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_set, batch_size=5, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "QcXW9oX-2U3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stratified_gender_model = train_model(train_loader, valid_loader, num_classes)"
      ],
      "metadata": {
        "id": "_iUzuAmK27ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stratified_gender_results = test_model(stratified_gender_model, test_loader, classes)"
      ],
      "metadata": {
        "id": "rkw-yitS2-iG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Race"
      ],
      "metadata": {
        "id": "APvSw5Te3BJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = BonyAnatomyJointSegmentationDataset(data_save_location, train_all.id, num_classes,\n",
        "                                                transforms=augmentations)\n",
        "\n",
        "valid_set = BonyAnatomyJointSegmentationDataset(data_save_location, valid_all.id, num_classes,\n",
        "                                                transforms=augmentations)\n",
        "\n",
        "test_set = BonyAnatomyJointSegmentationDataset(data_save_location,test_all.id, num_classes,\n",
        "                                               transforms=augmentations)\n",
        "\n",
        "train_loader = DataLoader(train_set, num_workers=2, batch_sampler=StratifiedSampler(train_all.P02RACE, batch_size=16))\n",
        "valid_loader = DataLoader(valid_set, batch_size=16, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_set, batch_size=5, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "pNhGw92A3CDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stratified_race_model = train_model(train_loader, valid_loader, num_classes)"
      ],
      "metadata": {
        "id": "0V-vHw8B3Eq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stratified_race_results = test_model(stratified_race_model, test_loader, classes)"
      ],
      "metadata": {
        "id": "wnQhEyzv3H1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Model Evaluation using Intersection over Union (IoU)"
      ],
      "metadata": {
        "id": "7wLJyTBiIUcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_bias(model, dataset, metric, num_classes = 9):\n",
        "    results = torch.zeros((1, num_classes))\n",
        "    test_loader = generate_dataloader(dataset, num_classes)\n",
        "    for x, y in test_loader:\n",
        "        results += metric(torch.softmax(model(x.cuda()), dim=1), y.cuda()).detach().cpu()\n",
        "\n",
        "    return results/len(test_loader)\n",
        "\n",
        "def generate_dataloader(dataset, num_classes):\n",
        "    augmentations = A.Compose([A.Resize(256, 256), ToTensorV2()]) # A.OneOf([A.Emboss(), Canny()\n",
        "    test_set = BonyAnatomyJointSegmentationDataset(data_save_location, dataset.id, num_classes,\n",
        "                                                   transforms=augmentations)\n",
        "    test_loader = DataLoader(test_set, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "    return test_loader"
      ],
      "metadata": {
        "id": "XW7id1fHI3n2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multi_jaccard = MulticlassJaccardIndex(num_classes=9, average=\"none\").cuda()\n",
        "multi_jaccard.__name__ = \"iou\""
      ],
      "metadata": {
        "id": "S9tiKIDKJBeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_records = pd.read_csv(data_location)"
      ],
      "metadata": {
        "id": "PtSiyDvEIjxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "male_group = data_records[data_records.P02SEX == \"1: Male\"].reset_index(drop=True)\n",
        "female_group = data_records[data_records.P02SEX == \"2: Female\"].reset_index(drop=True)\n",
        "white_caucasian_group = data_records[data_records.P02RACE == \"1: White or Caucasian\"].reset_index(drop=True)\n",
        "black_aa_group = data_records[data_records.P02RACE == \"2: Black or African American\"].reset_index(drop=True)"
      ],
      "metadata": {
        "id": "5dViCGL1IWOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Baseline Models"
      ],
      "metadata": {
        "id": "1445uVxuSo9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gender_male_metrics_baseline = eval_bias(baseline_model, male_group, multi_jaccard)\n",
        "gender_female_metrics_baseline = eval_bias(baseline_model, female_group, multi_jaccard)\n",
        "race_white_metrics_baseline = eval_bias(baseline_model, white_caucasian_group, multi_jaccard)\n",
        "race_black_metrics_baseline = eval_bias(baseline_model, black_aa_group, multi_jaccard)\n",
        "\n",
        "base_dict = {\n",
        "    \"Gender: Male\": gender_male_metrics_baseline.numpy().squeeze(),\n",
        "    \"Gender: Female\": gender_female_metrics_baseline.numpy().squeeze(),\n",
        "    \"Race: White/Caucasian\": race_white_metrics_baseline.numpy().squeeze(),\n",
        "    \"Race: Black/AA\": race_black_metrics_baseline.numpy().squeeze(),\n",
        "}\n",
        "\n",
        "baseline_res = pd.DataFrame(base_dict).T\n",
        "baseline_res[\"Average\"] = baseline_res.mean(axis=1)\n",
        "baseline_res.columns = [val for key, val in classes.items()] + [\"Average\"]\n",
        "baseline_res"
      ],
      "metadata": {
        "id": "B6-6QFQlIwQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Balanced Models"
      ],
      "metadata": {
        "id": "mfUbgOYjSrh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gender_male_metrics_balanced = eval_bias(balanced_gender_model, male_group, multi_jaccard)\n",
        "gender_female_metrics_balanced = eval_bias(balanced_gender_model, female_group, multi_jaccard)\n",
        "\n",
        "balanced_gender = {\n",
        "    \"Gender: Male\": gender_male_metrics_balanced.numpy().squeeze(),\n",
        "    \"Gender: Female\": gender_female_metrics_balanced.numpy().squeeze(),\n",
        "}\n",
        "balanced_gender_res = pd.DataFrame(balanced_gender).T\n",
        "balanced_gender_res[\"Average\"] =  balanced_gender_res.mean(axis=1)\n",
        "balanced_gender_res.columns = [val for key, val in classes.items()] + [\"Average\"]\n",
        "\n",
        "balanced_gender_res"
      ],
      "metadata": {
        "id": "b0N3EhkmKFjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "race_white_metrics_balanced = eval_bias(balanced_race_model, white_caucasian_group, multi_jaccard)\n",
        "race_black_metrics_balanced = eval_bias(balanced_race_model, black_aa_group, multi_jaccard)\n",
        "\n",
        "balanced_race = {\n",
        "     \"Race: White/Caucasian\": race_white_metrics_balanced.numpy().squeeze(),\n",
        "    \"Race: Black/AA\": race_black_metrics_balanced.numpy().squeeze(),\n",
        "}\n",
        "balanced_race_res = pd.DataFrame(balanced_race).T\n",
        "balanced_race_res[\"Average\"] = balanced_race_res.mean(axis=1)\n",
        "balanced_race_res.columns = [val for key, val in classes.items()] + [\"Average\"]\n",
        "\n",
        "balanced_race_res"
      ],
      "metadata": {
        "id": "Xm6YQ9RkKV5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Stratified Models"
      ],
      "metadata": {
        "id": "sjvp39QuSugU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gender_male_metrics_stratified = eval_bias(stratified_gender_model, male_group, multi_jaccard)\n",
        "gender_female_metrics_stratified = eval_bias(stratified_gender_model, female_group, multi_jaccard)\n",
        "\n",
        "strat_gender = {\n",
        "    \"Gender: Male\": gender_male_metrics_stratified.numpy().squeeze(),\n",
        "    \"Gender: Female\": gender_female_metrics_stratified.numpy().squeeze(),\n",
        "}\n",
        "strat_gender_res = pd.DataFrame(strat_gender).T\n",
        "strat_gender_res[\"Average\"] = strat_gender_res.mean(axis=1)\n",
        "strat_gender_res.columns = [val for key, val in classes.items()] + [\"Average\"]\n",
        "strat_gender_res"
      ],
      "metadata": {
        "id": "UY0j5j6zK_yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "race_white_metrics_stratified = eval_bias(stratified_race_model, white_caucasian_group, multi_jaccard)\n",
        "race_black_metrics_stratified= eval_bias(stratified_race_model, black_aa_group, multi_jaccard)\n",
        "\n",
        "strat_race = {\n",
        "    \"Race: White/Caucasian\": race_white_metrics_stratified.numpy().squeeze(),\n",
        "    \"Race: Black/AA\": race_black_metrics_stratified.numpy().squeeze(),\n",
        "}\n",
        "strat_race_res = pd.DataFrame(strat_race).T\n",
        "strat_race_res[\"Average\"] = strat_race_res.mean(axis=1)\n",
        "strat_race_res.columns = [val for key, val in classes.items()] + [\"Average\"]\n",
        "strat_race_res"
      ],
      "metadata": {
        "id": "Xd-7qEYTK3EI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Group Specific Models"
      ],
      "metadata": {
        "id": "5ENL2aPZSyD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gender_male_metrics = eval_bias(baseline_male_model, male_group, multi_jaccard)\n",
        "\n",
        "gender_male = {\n",
        "    \"Gender: Male\": gender_male_metrics.numpy().squeeze(),\n",
        "}\n",
        "gender_male = pd.DataFrame(gender_male).T\n",
        "gender_male[\"Average\"] = gender_male.mean(axis=1)\n",
        "gender_male.columns = [val for key, val in classes.items()] + [\"Average\"]\n",
        "gender_male"
      ],
      "metadata": {
        "id": "6X_5WxHmLKTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gender_female_metrics = eval_bias(baseline_female_model, female_group, multi_jaccard)\n",
        "\n",
        "gender_female = {\n",
        "    \"Gender: Female\": gender_female_metrics.numpy().squeeze(),\n",
        "}\n",
        "gender_female = pd.DataFrame(gender_female).T\n",
        "gender_female[\"Average\"] = gender_female.mean(axis=1)\n",
        "gender_female.columns = [val for key, val in classes.items()] + [\"Average\"]\n",
        "gender_female"
      ],
      "metadata": {
        "id": "8wxnEBqRLUyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "race_white_metrics = eval_bias(baseline_white_model, white_caucasian_group, multi_jaccard)\n",
        "\n",
        "race_white = {\n",
        "    \"Race: White/Caucasian\": race_white_metrics.numpy().squeeze(),\n",
        "}\n",
        "race_white = pd.DataFrame(race_white).T\n",
        "race_white[\"Average\"] = race_white.mean(axis=1)\n",
        "race_white.columns = [val for key, val in classes.items()] + [\"Average\"]\n",
        "race_white"
      ],
      "metadata": {
        "id": "Lf31YIEZLcEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "race_black_metrics = eval_bias(baseline_black_model, black_aa_group, multi_jaccard)\n",
        "\n",
        "race_black = {\n",
        "    \"Race: Black/AA\": race_black_metrics.numpy().squeeze(),\n",
        "}\n",
        "race_black = pd.DataFrame(race_black).T\n",
        "race_black[\"Average\"] = race_black.mean(axis=1)\n",
        "race_black.columns = [val for key, val in classes.items()] + [\"Average\"]\n",
        "race_black"
      ],
      "metadata": {
        "id": "pNeHkyERLpNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Fairness Evaluation"
      ],
      "metadata": {
        "id": "tzeM8aghSYaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_fairness_score(data):\n",
        "    sd = []\n",
        "    ser = []\n",
        "\n",
        "    for i, row in data.iterrows():\n",
        "        sd.append(row[1:].std())\n",
        "        min_group = row[1:].min()\n",
        "        max_group = row[1:].max()\n",
        "        ser.append((1 - min_group)/(1 - max_group))\n",
        "\n",
        "    return sd, ser"
      ],
      "metadata": {
        "id": "R_DvIH88L5WB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Gender"
      ],
      "metadata": {
        "id": "uJx5N5Y9Sceh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fairness_gender = {\n",
        "    \"Model\": [\"Baseline\", \"Balanced\", \"Stratified\", \"Group-Specific\"],\n",
        "    \"Male\": [baseline_res.loc[\"Gender: Male\", \"Average\"],\n",
        "             balanced_gender_res.loc[\"Gender: Male\", \"Average\"],\n",
        "             strat_gender_res.loc[\"Gender: Male\", \"Average\"],\n",
        "             gender_male.loc[\"Gender: Male\", \"Average\"]],\n",
        "    \"Female\":[baseline_res.loc[\"Gender: Female\", \"Average\"],\n",
        "             balanced_gender_res.loc[\"Gender: Female\", \"Average\"],\n",
        "             strat_gender_res.loc[\"Gender: Female\", \"Average\"],\n",
        "             gender_female.loc[\"Gender: Female\", \"Average\"]]\n",
        "}\n",
        "\n",
        "fairness_df_gender = pd.DataFrame(fairness_gender)\n",
        "sd, ser = calc_fairness_score(fairness_df_gender)\n",
        "fairness_df_gender[\"SD\"] = sd\n",
        "fairness_df_gender[\"SER\"] = ser\n",
        "\n",
        "fairness_df_gender"
      ],
      "metadata": {
        "id": "ozSgh74AMA20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Race"
      ],
      "metadata": {
        "id": "xVeknCuzShO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fairness_race = {\n",
        "    \"Model\": [\"Baseline\", \"Balanced\", \"Stratified\", \"Group-Specific\"],\n",
        "    \"White/Caucasian\": [baseline_res.loc[\"Race: White/Caucasian\", \"Average\"],\n",
        "             balanced_race_res.loc[\"Race: White/Caucasian\", \"Average\"],\n",
        "             strat_race_res.loc[\"Race: White/Caucasian\", \"Average\"],\n",
        "             race_white.loc[\"Race: White/Caucasian\", \"Average\"]],\n",
        "    \"Black/African American\":[baseline_res.loc[\"Race: Black/AA\", \"Average\"],\n",
        "             balanced_race_res.loc[\"Race: Black/AA\", \"Average\"],\n",
        "             strat_race_res.loc[\"Race: Black/AA\", \"Average\"],\n",
        "             race_black.loc[\"Race: Black/AA\", \"Average\"]]\n",
        "}\n",
        "\n",
        "fairness_df_race = pd.DataFrame(fairness_race)\n",
        "sd, ser = calc_fairness_score(fairness_df_race)\n",
        "fairness_df_race[\"SD\"] = sd\n",
        "fairness_df_race[\"SER\"] = ser\n",
        "\n",
        "fairness_df_race"
      ],
      "metadata": {
        "id": "PFAyOnhAMEN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ANz0PcXYNmsB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}