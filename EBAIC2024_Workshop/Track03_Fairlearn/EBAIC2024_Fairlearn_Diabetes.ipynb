{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNN8zxHJ4RQ9xhdMynZtv5B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pitthexai/IEEE_ICHI_EBAICWorkshop/blob/main/EBAIC2024_Workshop/Track03_Fairlearn/EBAIC2024_Fairlearn_Diabetes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EBAIC 2024 Track III: Fairlearn: An open-source package to improve fairness of AI\n",
        "\n",
        "The field of fairness in AI systems is an interdisciplinary area of research and practice focused on understanding and mitigating the negative impacts of AI on society. In this tutorial, we utilize Fairlearn -- an open-source library designed to help improve the fairness of AI systems. In this tutorial, we consider an automated system for recommending patients for high-risk care management programs\n",
        "\n",
        "## Dataset and Task\n",
        "Using a dataset included with the Fairlearn library, we will be working with a clincial dataset of containing re-admissions over a ten-year period (1998-2008) for diabetic patients across 130 different hospitals within the US. Features included within the dataset include:\n",
        "\n",
        "- demographics,\n",
        "- diagnoses,\n",
        "- diabetic medications,\n",
        "- number of visits in the year preceding the encounter,\n",
        "- payer information,\n",
        "- whether the patient was readmitted after release,\n",
        "- whether the readmission occurred within 30 days of the release\n",
        "\n",
        "Out goal is to develop a classification model that decides whether the patients should be suggested to their primary care physicians for an enrollment into a high-risk care management program."
      ],
      "metadata": {
        "id": "p6DZ3AccQjph"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Package Setup"
      ],
      "metadata": {
        "id": "bE8h3xNMQzEI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "en1pUdd1MdgD"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade fairlearn==0.10.0\n",
        "!pip install --upgrade scikit-learn\n",
        "!pip install --upgrade seaborn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "pd.set_option(\"display.float_format\", \"{:.3f}\".format)"
      ],
      "metadata": {
        "id": "tcot9f6bNR9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set()"
      ],
      "metadata": {
        "id": "VBlJPnnsN8NN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.utils import Bunch\n",
        "from sklearn.metrics import (\n",
        "    balanced_accuracy_score,\n",
        "    roc_auc_score,\n",
        "    accuracy_score,\n",
        "    recall_score,\n",
        "    confusion_matrix,\n",
        "    roc_auc_score,\n",
        "    roc_curve)\n",
        "from sklearn import set_config\n",
        "\n",
        "set_config(display=\"diagram\")"
      ],
      "metadata": {
        "id": "bh9iQU_JN972"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fairlearn.metrics import (\n",
        "    MetricFrame,\n",
        "    true_positive_rate,\n",
        "    false_positive_rate,\n",
        "    false_negative_rate,\n",
        "    selection_rate,\n",
        "    count,\n",
        "    false_negative_rate_difference\n",
        ")\n",
        "\n",
        "from fairlearn.datasets import fetch_diabetes_hospital\n",
        "from fairlearn.postprocessing import ThresholdOptimizer, plot_threshold_optimizer\n",
        "from fairlearn.postprocessing._interpolated_thresholder import InterpolatedThresholder\n",
        "from fairlearn.postprocessing._threshold_operation import ThresholdOperation\n",
        "from fairlearn.reductions import ExponentiatedGradient, EqualizedOdds, TruePositiveRateParity"
      ],
      "metadata": {
        "id": "uXOy3yYGOAO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Exploration\n",
        "\n",
        "The first step is to explore the data provided for any fairness issues that may occur. Specifically, we look at:\n",
        "1. sample sizes of different demographic groups, and in particular different racial groups\n",
        "2. balance of the class labels"
      ],
      "metadata": {
        "id": "wItEp0DL2eZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the Dataset\n",
        "We first load the dataset using the Fairlearn library. We then construct our target column ```readmit_30_days```."
      ],
      "metadata": {
        "id": "Of49gzc032uu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes_data = fetch_diabetes_hospital()"
      ],
      "metadata": {
        "id": "N3aLa3S3OIZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes_df = diabetes_data.data"
      ],
      "metadata": {
        "id": "mg696_ZA4AvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes_df[\"readmit_30_days\"] = np.where(diabetes_df.readmitted == \"<30\", 1, 0)"
      ],
      "metadata": {
        "id": "OWcvuSYm5QVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Group Sizes\n",
        "\n",
        "For assessing fairness, a key data characteristic is the sample size of groups we are conducting a fairness assessment for. Small sample sizes have two key implications:\n",
        "\n",
        "- **assessment**: Smaller groups are harder to assess due to fewer data points, which leads to a much larger uncertainty in our estimates\n",
        "\n",
        "- **model training**: fewer training data points can cause our model to not appropriately capture any data patterns specific to smaller groups. This can lead to worse predictive performance on these groups.\n"
      ],
      "metadata": {
        "id": "qBn2_U5u9UTB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Race Group Sizes"
      ],
      "metadata": {
        "id": "9y5ExmuM-cbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes_df[\"race\"].value_counts()"
      ],
      "metadata": {
        "id": "fWAXM6Tw6Yle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes_df[\"race\"].value_counts().plot(kind='bar', rot=45);"
      ],
      "metadata": {
        "id": "JmtA_WUw6sdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes_df[\"race\"].value_counts(normalize=True) # frequencies"
      ],
      "metadata": {
        "id": "Omc63cRatI7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop gender group Unknown/Invalid\n",
        "diabetes_df = diabetes_df.query(\"gender != 'Unknown/Invalid'\")\n",
        "\n",
        "# retain the original race as race_all, and merge Asian+Hispanic+Other\n",
        "diabetes_df[\"race_all\"] = diabetes_df[\"race\"]\n",
        "diabetes_df[\"race\"] = diabetes_df[\"race\"].replace({\"Asian\": \"Other\", \"Hispanic\": \"Other\"})"
      ],
      "metadata": {
        "id": "kt6wiV5EtZU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Gender"
      ],
      "metadata": {
        "id": "0IMKJtUy-vCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes_df[\"gender\"].value_counts()"
      ],
      "metadata": {
        "id": "U0XLtIvv-pbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes_df[\"gender\"].value_counts().plot(kind='bar', rot=45);"
      ],
      "metadata": {
        "id": "RX56UTOA-ygV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Label Imbalance\n",
        "\n",
        "We next look at the frequency of our class labels. The frequency of the labels is important because:\n",
        "- some classification algorithms and evaluation metrics won't work well with data sets that contain extreme class imbalances\n",
        "- extreme class imbalance may make bias towards certain groups worse due to smaller group sizes in fairness assessment"
      ],
      "metadata": {
        "id": "pse5PqOmrwYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes_df[\"readmit_30_days\"].value_counts()"
      ],
      "metadata": {
        "id": "fE_nBwlU-6pW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Due to the large imbalance between the negative and positive class, we will use balanced accuracy to evaluate our predictive model."
      ],
      "metadata": {
        "id": "dWhRlzLAvyUr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a Model\n",
        "Next, we train a a classification model. Here, we utilize logistic regression for both its interpretability and the model expresiveness."
      ],
      "metadata": {
        "id": "KAlehjpgrvv4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training/Test Splits\n",
        "We split the data into train/test splits with a 50/50 split. Because our evaluation metric is balanced accuracy, we will resample the data set to have the same number of positive and negative examples for training."
      ],
      "metadata": {
        "id": "JEWkYuP0uqwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_variable = \"readmit_30_days\"\n",
        "demographic = [\"race\", \"gender\"]\n",
        "sensitive = [\"race\"]"
      ],
      "metadata": {
        "id": "gFxYn8Qstt6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_variable = \"readmit_30_days\"\n",
        "demographic = [\"race\", \"gender\"]\n",
        "sensitive = [\"race\"]\n",
        "\n",
        "Y, A = diabetes_df.loc[:, target_variable], diabetes_df.loc[:, sensitive]\n",
        "\n",
        "X = pd.get_dummies(diabetes_df.drop(columns=[\n",
        "    \"race\",\n",
        "    \"race_all\",\n",
        "    \"discharge_disposition_id\",\n",
        "    \"readmitted\",\n",
        "    \"readmit_binary\",\n",
        "    \"readmit_30_days\"\n",
        "]))"
      ],
      "metadata": {
        "id": "U0sVFFw4uvZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_seed = 45\n",
        "np.random.seed(random_seed)\n",
        "\n",
        "X_train, X_test, Y_train, Y_test, A_train, A_test, df_train, df_test = train_test_split(\n",
        "    X,\n",
        "    Y,\n",
        "    A,\n",
        "    diabetes_df,\n",
        "    test_size=0.50,\n",
        "    stratify=Y,\n",
        "    random_state=random_seed\n",
        ")"
      ],
      "metadata": {
        "id": "191zluvku0J1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resample_dataset(X_train, Y_train, A_train):\n",
        "\n",
        "  negative_ids = Y_train[Y_train == 0].index\n",
        "  positive_ids = Y_train[Y_train == 1].index\n",
        "  balanced_ids = positive_ids.union(np.random.choice(a=negative_ids, size=len(positive_ids)))\n",
        "\n",
        "  X_train = X_train.loc[balanced_ids, :]\n",
        "  Y_train = Y_train.loc[balanced_ids]\n",
        "  A_train = A_train.loc[balanced_ids, :]\n",
        "  return X_train, Y_train, A_train"
      ],
      "metadata": {
        "id": "-S_RmE0DvV0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_bal, Y_train_bal, A_train_bal = resample_dataset(X_train, Y_train, A_train)"
      ],
      "metadata": {
        "id": "96eZntMGwhil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression Model"
      ],
      "metadata": {
        "id": "b0Xdkr7Uwmq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unmitigated_pipeline = Pipeline(steps=[\n",
        "    (\"preprocessing\", StandardScaler()),\n",
        "    (\"logistic_regression\", LogisticRegression(max_iter=1000))\n",
        "])"
      ],
      "metadata": {
        "id": "AeX6hEWYwjFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unmitigated_pipeline.fit(X_train_bal, Y_train_bal)"
      ],
      "metadata": {
        "id": "xuKUMZnVwqdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred_proba = unmitigated_pipeline.predict_proba(X_test)[:,1]\n",
        "Y_pred = unmitigated_pipeline.predict(X_test)"
      ],
      "metadata": {
        "id": "A8URR6bQwsnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_accuracy_score(Y_test, Y_pred)"
      ],
      "metadata": {
        "id": "Mq6pdp0Jwu8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coef_series = pd.Series(data=unmitigated_pipeline.named_steps[\"logistic_regression\"].coef_[0], index=X.columns)\n",
        "coef_series.sort_values().plot.barh(figsize=(4, 12), legend=False);"
      ],
      "metadata": {
        "id": "CudDwRwewxN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fairness Assessment\n",
        "\n",
        "In the healthcare scenario, when patients who can benefit from a care management program but are not recommended, they experience allocation harm. In classification, these patients are referred to as false negatives. Here, we focus on groups defined by race.\n",
        "\n",
        "To evaluate the fairness we use two metrics to quantify the harms and benefits:\n",
        "- **false negative rates (quantifying harm)**: the fraction of patients that are readmitted within 30 days, but that are not recommended for the care management program\n",
        "- **selection rate (quantifying benefits)**: the overall fraction of patients that are recommended for the care management program\n",
        "\n",
        "To easily compare false negative rate across groups defined by race we report group specific false negative rates as well as the largest distance, smallest ratio, and maximum worst-case false-negative rate.  "
      ],
      "metadata": {
        "id": "A1XejCRUxD4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can also evaluate multiple metrics by providing a dictionary\n",
        "\n",
        "metrics_dict = {\n",
        "    \"selection_rate\": selection_rate,\n",
        "    \"false_negative_rate\": false_negative_rate,\n",
        "    \"balanced_accuracy\": balanced_accuracy_score,\n",
        "}\n",
        "\n",
        "metricframe_unmitigated = MetricFrame(metrics=metrics_dict,\n",
        "                  y_true=Y_test,\n",
        "                  y_pred=Y_pred,\n",
        "                  sensitive_features=df_test['race'])\n",
        "\n",
        "# The disaggregated metrics are then stored in a pandas DataFrame:\n",
        "\n",
        "metricframe_unmitigated.by_group"
      ],
      "metadata": {
        "id": "6G9qikQwygHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame({'difference': metricframe_unmitigated.difference(),\n",
        "              'ratio': metricframe_unmitigated.ratio(),\n",
        "              'group_min': metricframe_unmitigated.group_min(),\n",
        "              'group_max': metricframe_unmitigated.group_max()}).T"
      ],
      "metadata": {
        "id": "HsQKXpKbzKqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metricframe_unmitigated.by_group.plot.bar(subplots=True, layout= [1,3], figsize=(12, 4),\n",
        "                      legend=False, rot=-45, position=1.5);"
      ],
      "metadata": {
        "id": "aMbCJZU2zZyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the plots, the Unknown groups is selected for the care management program less often than other groups and a larger fraction of group members that are likely to benefit from a care management program are not selected for it."
      ],
      "metadata": {
        "id": "x2XizOwHz3Ud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mitigating Fairness-related Harms through Postprocessing\n",
        "\n",
        "Postprocessing techniques are a class of unfairness-mitigation algorithms that take a trained model and a dataset as an input and fits a transformation function to model's outputs to satisfy some (group) fairness constraint(s), in our case the false negative rate. Here, we use the ```ThresholdOptimizer``` which uses a models predictions as a scoring function to identify a separate thrceshold for each sensitive group to optimize a specific metric. This metric is subject to specified fairness constraints. Here we use **false negative rate parity**, which requires that all the groups have equal values of false negative rate."
      ],
      "metadata": {
        "id": "MPlcEqkS0blF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we instantite ThresholdOptimizer with the logistic regression estimator\n",
        "postprocess_est = ThresholdOptimizer(\n",
        "    estimator=unmitigated_pipeline,\n",
        "    constraints=\"false_negative_rate_parity\",\n",
        "    objective=\"balanced_accuracy_score\",\n",
        "    prefit=True,\n",
        "    predict_method='predict_proba'\n",
        ")\n",
        "\n",
        "postprocess_est.fit(X_train_bal, Y_train_bal, sensitive_features=A_train_bal)"
      ],
      "metadata": {
        "id": "rQ2DGSdgzhXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred_postprocess = postprocess_est.predict(X_test, sensitive_features=A_test)\n",
        "metricframe_postprocess = MetricFrame(\n",
        "    metrics=metrics_dict,\n",
        "    y_true=Y_test,\n",
        "    y_pred=Y_pred_postprocess,\n",
        "    sensitive_features=A_test\n",
        ")"
      ],
      "metadata": {
        "id": "f9l20cZi1sFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.concat([metricframe_unmitigated.by_group,\n",
        "           metricframe_postprocess.by_group],\n",
        "           keys=['Unmitigated', 'ThresholdOptimizer'],\n",
        "           axis=1)"
      ],
      "metadata": {
        "id": "bhnunjiF1xL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.concat([metricframe_unmitigated.difference(),\n",
        "           metricframe_postprocess.difference()],\n",
        "          keys=['Unmitigated: difference', 'ThresholdOptimizer: difference'],\n",
        "          axis=1).T"
      ],
      "metadata": {
        "id": "8QjZXuiy1z7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metricframe_postprocess.by_group.plot.bar(subplots=True, layout=[1,3], figsize=(12, 4), legend=False, rot=-45, position=1.5)\n"
      ],
      "metadata": {
        "id": "5-FaWICc142g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Credit\n",
        "This tutorial is based off of the following notebook:\n",
        "[<br>_Fairness in AI systems: From social context to practice using Fairlearn_](https://colab.research.google.com/github/fairlearn/talks/blob/main/2021_scipy_tutorial/fairness-in-AI-systems-student.ipynb#scrollTo=Sch9KDWg7SL8)"
      ],
      "metadata": {
        "id": "GSDy0mp12ESQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wYB68lpr1_fp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}